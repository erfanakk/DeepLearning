## Intuition behind GANs
There are two ways to look at a GAN.
Call it an artist that sketches realistic images from scratch. And like many successful artists, it too feels the need of a mentor to reach higher levels of proficiency. Seen thus, a GAN consists of:
+ An artist, i.e., the Generator
+ And a mentor, i.e., the Discriminator
The Discriminator helps the Generator in generating realistic images from what is merely noise.

![image](https://cdn.mathpix.com/snip/images/NZlqo0leQhB4FRpc_9PDyxuwcMKpslmbK9wjCQso1TI.original.fullsize.png)




A GAN comprises
a Generator $\mathbf{G}$ and
a Discriminator $\mathbf{D}$, which are
trained simultaneously.
Given a dataset $X_{\text {real, }}$, the
generator $G$ tries to capture
the dataset distribution, by
producing images $X_{\text {fake }}$ from
noise $Z$. The
discriminator $D$ tries to
discriminate between the
original dataset
images $X_{\text {real }}$ and the images
produced by the generator $X_{\text {fake }}$ Through this adversarial process,  ==the end goal is to mimic the dataset distribution as realistically as possible. For instance, when provided with a dataset of car==

$x \quad \longrightarrow \quad$ Training Sample

$z \quad \longrightarrow \quad$ Noise Vector

$x_{\text {real }} \rightarrow$ Real Images

$x_{\text {fake }} \longrightarrow$ Output of the generator, $\mathrm{G}(\mathrm{Z})$

$D \quad$ Discriminator

$G(z)$ Or $x_{\text {fake }}^{\longrightarrow} \rightarrow$ Generator's output

$G(x) \in(0,1) \rightarrow$ Discriminator's ouptut


## Generator

Generator in GAN is a neural network, which given a random set of values, does a series of non-linear computations to produce real-looking images. The generator produces fake images Xfake,when fed a random vector Z, sampled from a multivariate-gaussian distribution.

![Generator](https://cdn.mathpix.com/snip/images/F_sm03G_mu4LnKbYYCL3sol3nSlqFCkMiz9k_Po4_qo.original.fullsize.png)


The generator’s role is to:

- Fool the discriminator
- Produce realistic-looking images
- Achieve high-performance as the training process complete

## Discriminator

The discriminator is based on the concept of discriminative modeling, which you learned is a classifier that tries to classify different classes in a dataset, with class-specific labels. So, in essence, it is similar to a supervised-classification problem. Also, the discriminator’s ability to classify observations is not limited to images, but includes video, text and many other domains (multi-modal).

![discriminator](https://cdn.mathpix.com/snip/images/n_cupzP60nz3DG3EwlXYKJMOkkd6BPeVEtGSy6DOUVw.original.fullsize.png)

The discriminator’s role in GAN is to solve a binary classification problem that learns to discriminate between a real and fake image. It does this by:

+ Predicting whether the observation is generated by the generator (fake), or from the original data distribution (real). 
+ While doing so, it learns a set of parameters or weights (theta). The weights keep getting updated, as the training progresses

## training

The training of the generator and discriminator in GAN is done in an alternating fashion. In the first step:

- The images produced by the generator Xfake and the original images Xreal are first passed to the discriminator.
- The discriminator then predicts Ypred ( a probability score ). This tells you which of the X images are real, and which fake. 
- Next, the predictions are compared with the ground truth { 0: fake, 1: real }, and a Binary Cross-Entropy (BCE) loss is calculated. 
- The loss (or gradient) is then backpropagated only through the discriminator, and its parameters are optimized accordingly.

**In the second step** 

- The generator produces images Xfake , which are again passed through the discriminator.
- Here too it outputs a prediction Ypred.
- And the BCE loss is computed. 
- Now, in this alternate step, because you want to enforce your Generator to produce images, as similar to the real images as possible (i.e., close to the true distribution), the true labels (or ground truth) are all labeled as ‘real’ or 1. As a result, when the generator tries to fool the discriminator (into believing that the images generated by it are real), the loss is backpropagated only through the generator network, and its parameters are optimized suitably.
